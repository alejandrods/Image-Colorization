{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.models import Sequential, Model\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len data:  2000\n"
     ]
    }
   ],
   "source": [
    "data = glob.glob('./data_faces/Train/*.jpg')\n",
    "# img = load_img('./data_faces/Train/0A9kTN.jpg')\n",
    "\n",
    "data = data[:2000]\n",
    "print('Len data: ', len(data))\n",
    "images = []\n",
    "for items in data:\n",
    "    images.append(img_to_array(load_img(items)))\n",
    "images = np.array(images, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len images:  2000\n",
      "Image shape:  (256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Len images: ', len(images))\n",
    "print('Image shape: ', images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0/255 : Indicates that we are using a 24-bit RGB color space. 0-255 for each color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = 1.0/255*images\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights from InceptionResNetV2\n",
    "InceptionResNetV2 is the most powerful classifier. Extract the classification layer and merge it with the output from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n",
    "\n",
    "Our model is the next one:\n",
    "https://github.com/baldassarreFe/deep-koalarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"full_network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 256, 256, 1)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 128, 128, 64)  640         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 128, 128, 128) 73856       conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 64, 64, 128)   147584      conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 64, 64, 256)   295168      conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 32, 32, 256)   590080      conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 32, 32, 512)   1180160     conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 32, 32, 512)   2359808     conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 1024, 1000)    0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 32, 32, 256)   1179904     conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 32, 32, 1000)  0           repeat_vector_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 32, 32, 1256)  0           conv2d_8[0][0]                   \n",
      "                                                                   reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 32, 32, 256)   321792      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 32, 32, 128)   295040      conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)   (None, 64, 64, 128)   0           conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 64, 64, 64)    73792       up_sampling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)   (None, 128, 128, 64)  0           conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 128, 128, 32)  18464       up_sampling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 128, 128, 16)  4624        conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 128, 128, 2)   290         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)   (None, 256, 256, 2)   0           conv2d_14[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,541,202\n",
      "Trainable params: 6,541,202\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_input = Input(shape=(1000,))\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to resize the image to fit into the Inception model. Then we use the preprocessor to format the pixel and color values according to the model. In the final step, we run it through the Inception network and extract the final layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscaled_rgb are images\n",
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    # Resize the images to (299, 299, 3), the Inception model works with this\n",
    "    # dimension\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    # Conver to array\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    # Preprocess the array to make it with this dimension (sample, 299, 299, 3)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    # Predict these images in Inception Model\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training data\n",
    "batch_size = 50\n",
    "\n",
    "# We change from RGB to LAB:\n",
    "# - L: lightness\n",
    "# - a: green-red\n",
    "# - b: blue-yellow\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(X_train, batch_size=batch_size):\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        embed = create_inception_embedding(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/21\n",
      "90/90 [==============================] - 473s - loss: 0.0691   \n",
      "Epoch 2/21\n",
      "90/90 [==============================] - 452s - loss: 0.0114   \n",
      "Epoch 3/21\n",
      "90/90 [==============================] - 460s - loss: 0.0114   \n",
      "Epoch 4/21\n",
      "90/90 [==============================] - 466s - loss: 0.0135   \n",
      "Epoch 5/21\n",
      "90/90 [==============================] - 461s - loss: 0.0106   \n",
      "Epoch 6/21\n",
      "90/90 [==============================] - 457s - loss: 0.0107   \n",
      "Epoch 7/21\n",
      "90/90 [==============================] - 455s - loss: 0.0107   \n",
      "Epoch 8/21\n",
      "90/90 [==============================] - 455s - loss: 0.0105   \n",
      "Epoch 9/21\n",
      "90/90 [==============================] - 456s - loss: 0.0106   \n",
      "Epoch 10/21\n",
      "90/90 [==============================] - 459s - loss: 0.0104   \n",
      "Epoch 11/21\n",
      "90/90 [==============================] - 456s - loss: 0.0117   \n",
      "Epoch 12/21\n",
      "90/90 [==============================] - 455s - loss: 0.0104   \n",
      "Epoch 13/21\n",
      "90/90 [==============================] - 459s - loss: 0.0102   \n",
      "Epoch 14/21\n",
      "90/90 [==============================] - 449s - loss: 0.0103   \n",
      "Epoch 15/21\n",
      "90/90 [==============================] - 473s - loss: 0.0104   \n",
      "Epoch 16/21\n",
      "90/90 [==============================] - 449s - loss: 0.0101   \n",
      "Epoch 17/21\n",
      "90/90 [==============================] - 456s - loss: 0.0102   \n",
      "Epoch 18/21\n",
      "90/90 [==============================] - 449s - loss: 0.0102   \n",
      "Epoch 19/21\n",
      "90/90 [==============================] - 462s - loss: 0.0102   \n",
      "Epoch 20/21\n",
      "90/90 [==============================] - 429s - loss: 0.0102   \n",
      "Epoch 21/21\n",
      "90/90 [==============================] - 386s - loss: 0.0102   \n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"log/\")\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], \n",
    "                    epochs=21, steps_per_epoch=90)\n",
    "\n",
    "# Save model# Save  \n",
    "model_json = model.to_json()\n",
    "with open(\"./Models/model_full_colorizer.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"./Models/model_full_colorizer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\skimage\\io\\_io.py:132: UserWarning: ./results_full/img_30.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\skimage\\io\\_io.py:132: UserWarning: ./results_full/img_236.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\skimage\\io\\_io.py:132: UserWarning: ./results_full/img_383.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n"
     ]
    }
   ],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir('./data_faces/Test/'):\n",
    "    color_me.append(img_to_array(load_img('./data_faces/Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations:\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    imsave(\"./results_full/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
